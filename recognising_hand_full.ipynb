{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T10:15:24.326268Z",
     "start_time": "2019-12-28T10:15:17.917788Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import imutils\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the below code has to be run in scripts as cv2 and jupyter have a problem with running cv2.show, it sort of just makes the whole notebook crash which is not ideal. I have put all of the code into this notebook because I wanted to show my thought process and explain clearly what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't too much actual code in this notebook but the convoluted nerual net will take a while to run. If you would want to load the cnn model directly I will include it in my git hub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini project I really wanted to create my own dataset mainly because I thought it would be really fun but also I wanted to get more practice using the cv2 library as it was not something we used in our studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a load of different iterations I found that there were a couple different ways of isolating the hand. I have included the method that I used to the most success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_path=\"/Users/danielchow/Downloads/videos_for_nueral_net/mp4s/1_finger_stationary.mp4\"\n",
    "cam = cv2.VideoCapture(video_file_path) \n",
    "folder_name='data'+'_'+video_file_path.split('/')[-1].split('_')[0]+'_'+video_file_path.split('/')[-1].split('_')[-1].split('.')[0]+'gaussian'\n",
    "fgbg= cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "try: \n",
    "      \n",
    "    # creating a folder named data \n",
    "    if not os.path.exists(folder_name): \n",
    "        os.makedirs(folder_name) \n",
    "  \n",
    "    # if not created then raise error \n",
    "except OSError: \n",
    "    print ('Error: Creating directory of data') \n",
    "  \n",
    "    # frame \n",
    "currentframe = 1\n",
    "  \n",
    "while(True): \n",
    "      \n",
    "    # reading from frame \n",
    "    ret,frame = cam.read() \n",
    "  \n",
    "    if ret: \n",
    "        # if video is still left continue creating images\n",
    "        name = './'+folder_name+'/frame' + str(currentframe) + '.jpg'\n",
    "        print ('Creating...' + name)\n",
    "        cropped = frame[100:1100,50:500]\n",
    "        height , width , layers = cropped.shape\n",
    "        new_h=int(height/2)\n",
    "        new_w=int(width/2)\n",
    "        resize = cv2.resize(cropped, (new_w, new_h))\n",
    "        fgmask=fgbg.apply(resize)\n",
    "        median=cv2.medianBlur(fgmask,5)\n",
    "        ret,thresh1 = cv2.threshold(median,20,1000,cv2.THRESH_BINARY)\n",
    "        \n",
    "\n",
    "        # writing the extracted images \n",
    "        cv2.imwrite(name, thresh1) \n",
    "  \n",
    "        #stop duplicate images\n",
    "        currentframe += 1\n",
    "    else: \n",
    "        break\n",
    "  \n",
    "    #Release all space and windows once done \n",
    "cam.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For someone who has not seen the cv2 library this may seem quite confusing so I will break it down slightly and explain some of the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True): \n",
    "      \n",
    "    # reading from frame \n",
    "    ret,frame = cam.read() \n",
    "  \n",
    "    if ret: \n",
    "    \n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a loop that loops over a video (in this case) and then breaks when the video is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ret: \n",
    "        # if video is still left continue creating images\n",
    "        name = './'+folder_name+'/frame' + str(currentframe) + '.jpg'\n",
    "        print ('Creating...' + name)\n",
    "        cropped = frame[100:1100,50:500]\n",
    "        height , width , layers = cropped.shape\n",
    "        new_h=int(height/2)\n",
    "        new_w=int(width/2)\n",
    "        resize = cv2.resize(cropped, (new_w, new_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code takes a frame from the video crops it so only the bottom half (where the hand is) is showing and then resizes the image so its only half as big. This is done so the neural network has less pixels to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgbg= cv2.createBackgroundSubtractorMOG2()\n",
    "fgmask=fgbg.apply(resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where it gets slightly more complicated. The MOG2 background subtractor looks at the previous frames in a video and sees where the difference is compared to the current video. Therefore if the only thing moving is your hand it turns everything but your hand black - this is very useful as we only care about what the hand is doing. Below is an example from the cv2 website on what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](https://docs.opencv.org/3.4/Background_Subtraction_Tutorial_Scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median=cv2.medianBlur(fgmask,5)\n",
    "ret,thresh1 = cv2.threshold(median,20,1000,cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply a median blur on the image. This computes the median of all the pixels in a window and then the middle pixel is given this value. This is very good at removing salt and pepper noise - which I found quite common when using MOG2 background subtractor. Below is an example of median blur being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](https://blog.photopea.com/wp-content/uploads/2016/09/head.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we apply a threshold to the image. Thresholding is quite a simple concept in that we state a value for a pixel and everything below that value is black and everything above that value is white. Some examples for different types of thresholding are shown below but in this case normal binary thresholding worked well. The only thing left to do now is save the image into a predefined folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](https://opencv-python-tutroals.readthedocs.io/en/latest/_images/threshold.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall I did a couple videos of myself doing various hand signals and obtained over 1300 pictures. The hand signals I picked were- 1 finger pointing, the peace sign and the okay sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![h](https://raw.githubusercontent.com/danch12/Images_for_neural_hands/master/Data/Train/1_finger/frame134.jpg \"1 finger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/danch12/Images_for_neural_hands/master/Data/Train/ok_sign/frame522.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/danch12/Images_for_neural_hands/master/Data/Train/peace_sign/frame115.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained our dataset of images we can use a nerual network to process them. I used the xception cnn as a base and then put a new layer on top for my classes. I will go into more detail in the read me. Additionally I used google collab which is why the training path may look a bit weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path='/content/drive/My Drive/pics_for_neural/Images_for_neural_hands/Data/Train'\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=keras.applications.xception.preprocess_input)\n",
    "\n",
    "\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(training_path,\n",
    "                                                  target_size=(224,224),\n",
    "                                                  color_mode='rgb',\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  seed=1,\n",
    "                                                shuffle=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path='/content/drive/My Drive/pics_for_neural/Images_for_neural_hands/Data/Validation'\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  preprocessing_function=keras.applications.xception.preprocess_input)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_path,\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical',\n",
    "    \n",
    "    seed=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(3, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Setup a model checkpoint to save our best model\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '/content/drive/My Drive/Capstone/models/{epoch:02d}-{val_accuracy:.2f}.h5',\n",
    "        monitor=  'val_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "#also created an early stopping, both measures to help stop overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                               min_delta=0,\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               mode='auto',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to freeze the base layers as the new output layer is initialized randomly therefore could make large errors and therefore there will be a large error gradient that could destory the reused weights. To avoid this we freeze the weights until the new layer has been given some time to learn reasonable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SGD as it seems to have the best quality convergence\n",
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreezing the base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to do predictions in real time was actually relatively easy, the script was very similar to the one used in the data gathering stage with only a couple of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0:'1_finger',\n",
    "           1:'ok_sign',\n",
    "           2:'peace_sign'}\n",
    "\n",
    "def preprocess(image):\n",
    "    image = np.stack((image,) * 3, axis=-1)\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=load_model(\"/Users/danielchow/Downloads/videos_for_nueral_net/models/best_home_model.h5\")\n",
    "\n",
    "\n",
    "prob=None\n",
    "cl=None\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "bottomLeftCornerOfText = (100,200)\n",
    "fontScale              = 1\n",
    "fontColor              = (0,255,255)\n",
    "lineType               = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture(0) \n",
    "fgbg= cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "\n",
    "  \n",
    "    # frame \n",
    "currentframe = 0\n",
    "  \n",
    "while(True): \n",
    "      \n",
    "    # reading from frame \n",
    "    ret,frame = cam.read() \n",
    "\n",
    "    if ret: \n",
    "        \n",
    "\n",
    "\n",
    "        # resize the frame\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        cropped = frame[290:1100,200:1000]\n",
    "        height , width , layers = cropped.shape\n",
    "        new_h=int(height/2)\n",
    "        new_w=int(width/2)\n",
    "        resize = cv2.resize(cropped, (new_w, new_h))\n",
    "        fgmask=fgbg.apply(resize)\n",
    "        median=cv2.medianBlur(fgmask,5)\n",
    "        ret,thresh1 = cv2.threshold(median,20,1000,cv2.THRESH_BINARY)\n",
    "        print(thresh1.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if currentframe % 5 ==0:\n",
    "\n",
    "            img=preprocess(thresh1)\n",
    "            img=img/255\n",
    "            im2 = np.expand_dims(img, axis=0)\n",
    "            prob=max(model.predict(im2)[0])\n",
    "            cl=class_dict[np.argmax(model.predict(im2))]\n",
    "            font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            bottomLeftCornerOfText = (100,200)\n",
    "            fontScale              = 1\n",
    "            fontColor              = (0,255,255)\n",
    "            lineType               = 2\n",
    "\n",
    "\n",
    "\n",
    "        cv2.putText(frame,\n",
    "            f'{prob}-{cl}', \n",
    "        bottomLeftCornerOfText, \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "\n",
    "        # showing the \n",
    "        cv2.imshow('segment', thresh1) \n",
    "        cv2.imshow('real',frame)\n",
    "        #stop duplicate images\n",
    "        currentframe += 1\n",
    "\n",
    "         # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break \n",
    "    else: \n",
    "        break\n",
    "  \n",
    "    #Release all space and windows once done \n",
    "cam.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main changes in this script with the first being the if statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    image = np.stack((image,) * 3, axis=-1)\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image\n",
    "\n",
    "\n",
    "if currentframe % 5 ==0:\n",
    "\n",
    "            img=preprocess(thresh1)\n",
    "            img=img/255\n",
    "            im2 = np.expand_dims(img, axis=0)\n",
    "            prob=max(model.predict(im2)[0])\n",
    "            cl=class_dict[np.argmax(model.predict(im2))]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing function takes the image and turns it into a format that is suitable for the model. First the np.stack takes the image and broadcasts it across all three colour channels. Next the image is resized into the size that will be accepted by the xception model. Then the image is put though the special xception preprocessing function. Finally we scale the input pixels so all values are between 0 and 1 then exand the dimensions so it can be put into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model then outputs the class with the highest probability and what that probability is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only other major part that changes is the below bit which is needed for a video feed as it allows you to stop the video when you press a certain button. In this case q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_env",
   "language": "python",
   "name": "video_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
